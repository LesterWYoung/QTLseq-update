{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the QTL-Seq update notebook\n",
    "I'm planning on updating the QTL-Seq pipeline. This bioinformatics pipeline, written by Akira Abe, Iwate University, and published in [Takagaki et al, 2013](https://doi.org/10.1111/tpj.12105) performs bulked segreatant analysis using Next Generation Sequencing (NGS) data to identify QTL. We've identified a need to update the applications used in the pipeline and reduce the time and effort required to perform a run. \n",
    "\n",
    "In this notebook I'll provide shell scripts that I think could be used to update the current pipeline. At the same time I'll be providing an explaination of how I believe the original pipeline worked and how my suggested upgrades change the *efficiency* or the pipeline but doesn't affect the *concept* behind it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QTL-Seq requirements\n",
    "1. a reference sequence is absolutely required\n",
    "2. segregating population (F2 or RIL) and phenotype information of individuals in the population\n",
    "3. paired-end NGS sequence from the parents and individuals with contrasting phenotypes\n",
    "4. server to run pipeline on\n",
    "\n",
    "### Current software requirements\n",
    "- samtools v0.1.8\n",
    "- perl v5.8.8 (required to run the above)\n",
    "- perl module Math::Random::MT::Auto \n",
    "- bwa v0.5.9-r16 (although later versions have worked)\n",
    "- fastx-toolkit (last version was v0.0.14)\n",
    "- R v2.15.0 (later versions have worked)\n",
    "- coval package (downoaded in QTL-Seq download)\n",
    "\n",
    "### Proposed software requirements (version number are the ones used for this upgrade)\n",
    "- trimmomatic v0.38 (http://www.usadellab.org/cms/?page=trimmomatic)\n",
    "- samtools v1.9 **OR** bcftools v1.9 (probably the latter)\n",
    "- bowtie2 v2.3.4.3\n",
    "- R v3.5.2\n",
    "- perl ? I don't know if bcftools or bowtie2 require a version of perl to run\n",
    "- multi-core server will be required to run bcftools and bowtie2\n",
    "\n",
    "We should consider incorporating all these in a conda package (including perl if we want it to run in an specific environment?). Also, I'm not sure how to do things so that a user can run the pipeline on a server that used batch job submission such as Slurm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current pipeline structure\n",
    "The current home directory for QTL-Seq looks as follows. Click [here](#Proposed_modifications_to_the_pipeline) to skip to the proposed modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QTL-seq_test/\n",
      "├── 0.common\n",
      "├── 1.qualify_read\n",
      "├── 2.make_consensus\n",
      "│   ├── 10.bwa2bam\n",
      "│   ├── 20.coval_refine\n",
      "│   ├── 30.coval_call\n",
      "│   ├── 40.RYKMSWBDHV_to_ACGT\n",
      "│   ├── 50.make_consensus\n",
      "│   └── 90.align_to_this_fasta\n",
      "│       ├── 00.reference\n",
      "│       ├── 10.bwa2bam\n",
      "│       ├── 20.coval_refine\n",
      "│       └── 30.coval_call\n",
      "├── 3.alignment\n",
      "│   ├── 10.bwa2bam\n",
      "│   ├── 20.coval_refine\n",
      "│   ├── 30.coval_call\n",
      "│   ├── 40.exclude_common_snps\n",
      "│   └── 50.awk_custom\n",
      "├── 4.search_for_pair\n",
      "│   ├── 10.paired_or_unpaired\n",
      "│   ├── 20.search_for_pair_of_unpaired\n",
      "│   ├── 30.unpaired_to_paired\n",
      "│   └── 40.merge_paired\n",
      "├── 5.compare\n",
      "│   ├── 10.cbind_confidence_interval\n",
      "│   ├── 20.awk_custom2\n",
      "│   └── 90.slidingwindow\n",
      "│       ├── mut_index_2\n",
      "│       ├── mut_index_3\n",
      "│       ├── mut_index_4\n",
      "│       └── pngs\n",
      "├── Bat_make_common.fnc.sh\n",
      "├── config.txt\n",
      "├── downloaded_Coval\n",
      "├── downloaded_fasta\n",
      "├── execute_Coval\n",
      "└── ibrc_scripts\n"
     ]
    }
   ],
   "source": [
    "#not sure how to insert text from a file, but the following is the output from tree in the QTLseq directory\n",
    "cat QTL-Seq_dir_struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what happens in the pipeline\n",
    "\n",
    "### Requirements and initial setup\n",
    "\n",
    "The user needs to provide gzipped .fastq files (one for foward reads and one for reverse) for the following:\n",
    "   - one of the parents\n",
    "   - individuals in one bulk that have a known phenotype (e.g. disease susceptible, salt sensitive, short hieght)\n",
    "   - individuals of the contrasting phenotype (e.g. disease resistant, salt tolerant, tall hieght)\n",
    "\n",
    "Note:\n",
    "- The number of individuals in the contrasting pools needs to be the same.\n",
    "- Symbolic links to the read files can be used.\n",
    "- The read files must be gzipped .fastq files, separated into foward and reverse files\n",
    "- The .fastq file or it's symbolic needs to be in the appropriate directory (`parent`, `mybulk_A`, `mybulk_B`) in `1.qualify_read`\n",
    "- The following filename structure is required for the .fastq file or it's symbolic link. `name_[0-9]*_[12]_sequence.txt.gz` where `name` can be any text, `_[0-9]*` is a numeretical identifier for an individual (as many digits as required, `_[12]` refers to the forward or reverse reads. `_sequence.txt.gz` remains constant\n",
    "- The fastq header line for each read within the file requires an indicator at the end of the line (a space, /, or other character) followed by the read direction. \n",
    "\n",
    "Once data is uploaded and symbolic links are in place, the pipeline is ready to for configuration. The configuration information is in `config.txt` and can be edited by the user before running the pipeline. This file is used by `Bat_make_common.fnc.sh` to set \"global variables\".\n",
    "\n",
    "### 0.common\n",
    "The actual \"global variables\" are stored in `0.common/common.fnc` by `Bat_make_common.fnc.sh`. This file is loaded at the beginning of each shell script using ` . ../0.common/common.fnc` in the script itself\n",
    "\n",
    "The \"global variables\" will probably change with the updated pipeline. Go to [this link](#0.pipeline_variables) to see proposed modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.qualify_read\n",
    "The section is run by running `Run_all_Bats.sh <arg>`. The values for <arg> are 9 for the parent and 0 or 1 for the foward or reverse read pools. \n",
    "    First, the directory and file structure are established (`Bat_rename.sh`), quality filtering is performed (`Bat_fastq_quality_filter.sh`), statistics performed on each file and reads without a matching pair are removed for each read file. This latter step (`Bat_sep_pair.pl.sh`) uses a perl script (in `/ibrc_scripts/1./sep_pair3.pl`) to look at each read header line in the forward and reverse read files and removes any unpaired reads. Modifications to the perl script may be required if the end of the line signifying the /1 or /2 read is different (e.g. a # or / may be used, or it may be blank). Stats are run on the paired-reads-only files\n",
    "    The last step (`Bat_equalize_read.sh`) is only performed after both read pools (i.e. 0 and 1) have had the single-reads removed. The number of reads in each file is determined and then the number of reads in each individual in 0 is compared to the number of reads in the corresponding individual in 1 (need the same number of individuals in each pool). The number of reads in the larger file is reduced at random (requiring the perl module Math::Random::MT::Auto as it's able to handle large numbers) so the the individuals in 0 and 1 have the same number of reads.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. make_consensus\n",
    "There are two parts to this section. \n",
    "1. The first part (in `2.make_consensus`) generates a secondary sequence by aligning the parental reads against a reference sequence using `bwa aln` and then `bwa sampe`. The alignments are then sorted, duplicates removed and unmapped reads removed. The coval application is then used to fix misalignments in the read assembly and then it is used to generate a secondary reference with the original reference sequence as a base.\n",
    "2. The second part of this section (in `2.make_consensus/90.align_to_this_fasta`) aligns reads from each of the individuals in the two pools against the reference sequence. It uses `bwa align` and `bwa sampe` again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.search_for_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GitHub\n",
    "I'vejust started larning how to use git and GitHub. I'll use GitHub as the main repository and then use my Cabinet (university network drive) as my working directory. This means that when I work on updating the pipeline I'll fetch the latest version from GitHub, make the changes and then push them back to GitHub when I'm finished that session. This means that I can do the work on my laptop or work desktop.\n",
    "\n",
    "I'm setting the git repository up with the original pipeline as the master and then separate branches for each of the directories (0.common, 1.qualify_read etc).I think this wil be easier to see for version control and make it more simple to understand what changes are being made.\n",
    "\n",
    "I'll try to update versions of this notebook on GitHub as well so that veryone can see what I'm doing and provide hints/clarifications/help where needed!\n",
    "\n",
    "I just transferred this notebook to the git repository on cabinet. So lets see if that's a good idea. Maybe not as it might get confusing to keep updating things.\n",
    "\n",
    "It doesn't seem to be working very well to have the jupyter notebook in the pipeline directory as the links within the notebook don't work (although it looks good as GitHub can display markdown (.md) files nicely. I'll put it in the next directory above this and then save the notebook in the master branch using `jupyter nbconvert --to html QTL-Seq\\ update\\ notebook.ipynb`\n",
    "\n",
    "Update 1 march: I am putting the jupyter notebook in the master branch of the repository. To access it, I need to make sure I'm in the master branch and then to `jupyter notebook`\n",
    "\n",
    "Instead of trying to have a single clone of the GitHub repository on GitHub, I'm going to clone the GitHub repository to a directory on the local computer and work on it from there. After finishing something I'll push the changes (commits) back to GitHub and that will contain the updates, accessible from anywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Proposed_modifications_to_the_pipeline\n",
    "Modifications to the pipeline are detailed below\n",
    "- 0.common &rightarrow; [0.common](#0.pipeline_variables) (click [here](#0.common) to see description of original section)\n",
    "- 1.qualify_read &rightarrow; [1.trim_and_filter_reads](#1.trim_and_filter_reads) (click [here](#1.qualify_read) to see description of original section)\n",
    "- 2.make_consensus &rightarrow; [2.assign_reference](#2.assign_reference) click [here](#2.make_consensus) to see description of original section)\n",
    "- 2.make_consensus/90.align_to_this_fasta &rightarrow; [2-90.align_to_reference](#2-90.align_to_reference)\n",
    "\n",
    "### Overview of modifications\n",
    "1. Use trimmomatic instead of fastx_toolkit to:\n",
    "    - filter reads based on quality\n",
    "    - generate fowards and reverse files with paired reads\n",
    "    - speed up this step by using multiple threads\n",
    "    - requires fewer steps and generates fewer files\n",
    "    - trimmomatic can also tell if reads use old or new system for phred scores (e.g. -33 or -64)\n",
    "2. Use bowtie2 instead of bwa aln and bwa sampe\n",
    "    - can align paired end reads, reducing the number of files generated\n",
    "    - combines alignment of reads and determination of concordance in one step\n",
    "    - may be better for aligning plant genomes (handles repeats better)\n",
    "3. Use bcftools instead of samtools 0.1.8\n",
    "    - more recent app\n",
    "    - can use mpileup to call SNPs across samples\n",
    "    - possible to use INDELs as well as SNPs\n",
    "    - more stringent filtering can be applied to reduce number of SNPs resulting from misalignments.\n",
    "    - read depth (DP) can be used to select only SNPs with significant read depth\n",
    "    - alignment correction, SNP calling and filtering could be speeded up by using multiple threads\n",
    "    - DISADVANTAGE: SNP-index will need to be determined differently and calculated from genotype information in .bcf file\n",
    "    - DISADVANTAGE: an additional step may be required to generate files for R script\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top directory changes\n",
    "Add Parental_reads, Pool_A_reads and Pool_B_reads directory for storing read files for the parents and individuals from the pools. I put README.txt files in these directories\n",
    "Started \"Running_the_QTL-seq_pipeline.txt\" file into QTL-seq directory. This will be updated as I make the updates\n",
    "\n",
    "Generating a secondary reference is now optional (personal communication w. A. Abe).\n",
    "\n",
    "Put read files for one of the parents in the Parental_reads directory using `name_R[12].fastq.gz` format, if a secondary reference is being generated by the pipeline. If the secondary reference isn't goinfg to be generated, then leave this directory as it is.\n",
    "\n",
    "Put read-files for the individuals in the pools in Pool_A_reads and Pool_B_reads directories\n",
    "Use read file naming convention to `name_[0-9]*_R[12].fastq.gz`\n",
    "\n",
    "Helpful hint: if your read files are uncompressed bam format (.ubam or .bam), they can be convereted to fastq format using  bedtools (https://bedtools.readthedocs.io/) and the command `bedtools bamtofastq -i <BAM> -fq <out.R1.fastq.gz> -fq2 <out.R2.fastq.gz>`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.pipeline_variables\n",
    "Set varaiables for pipeline by running `sh Bat_make_common.fnc.sh` in top directory. It will adjust `0.common/common.fnc`\n",
    "\n",
    "Update 25 Feb. Decided not to change directory names as this will make things more difficult later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.trim_and_filter_reads\n",
    "Generate a list of read file names. Add the following lines to \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the difficulties I had with the original QTL-seq pipeline were to do with the 1.qualify_read section of the pipeline. This section also takes quite a while to perform as there are multiple steps and fastx_toolkit runs using a single thread on a single read-direction file (e.g. on forward reads files then reverse read files). Additionally, multiple `gunzip -c` steps are required as fastx_toolkit cannot use gzipped files.\n",
    "General format for trimmomatics is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while read -r filefromlist\n",
    "do\n",
    "    filenamefwd=\"name_${linenum}_1_sequence.txt.gz\"\n",
    "    filenamerev=\"name_${linenum}_2_sequence.txt.gz\"\n",
    "    \n",
    "    java -jar trimmomatic-0.36.jar PE -threads 4 $filenamefwd $filenamerev \\\n",
    "        $pairedfilenamefwd $unpairedfilenamefwd $pairedfilenamerev $unpairedfilenamerev \\\n",
    "        LEADING:30 TRAILING:30 SLIDINGWINDOW:5:30 MINLEN:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.assign_reference\n",
    "I've make a bowtie2 alignment script for aligning the parental reference against the reference sequence. It was written on my local computer so I'll need to modify it for the piepline (use absolute directory to navigate to correct location of readfiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-90.align_to_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
